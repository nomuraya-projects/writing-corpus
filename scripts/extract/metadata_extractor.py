#!/usr/bin/env python3
"""
FC2è¨˜äº‹ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã—ã¦metadata.jsonã‚’ç”Ÿæˆã™ã‚‹

ç›®çš„: 660ä»¶ã®FC2è¨˜äº‹ã‹ã‚‰ä»¥ä¸‹ã‚’æŠ½å‡º
- åŸºæœ¬æƒ…å ±ï¼ˆid, title, date, category, word_countï¼‰
- ã‚³ãƒ¼ãƒ‘ã‚¹ç”¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆquality_score, elo_ratingç­‰ï¼‰
- ãƒªãƒ©ã‚¤ãƒˆçŠ¶æ…‹ï¼ˆstatus, scoreç­‰ï¼‰

ä½¿ã„æ–¹: python3 metadata_extractor.py
å‡ºåŠ›: data/corpus/metadata.json
"""

import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional


def extract_frontmatter(content: str) -> Dict[str, any]:
    """
    Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®frontmatterã‚’æŠ½å‡º

    Args:
        content: ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã®å†…å®¹

    Returns:
        frontmatterè¾æ›¸ï¼ˆtitle, date, original_idï¼‰
    """
    frontmatter_match = re.match(r'^---\n(.*?)\n---\n', content, re.DOTALL)
    if not frontmatter_match:
        return {}

    frontmatter_text = frontmatter_match.group(1)
    frontmatter = {}

    for line in frontmatter_text.split('\n'):
        if ':' in line:
            key, value = line.split(':', 1)
            key = key.strip()
            value = value.strip().strip('"\'')

            if key == 'date':
                frontmatter[key] = value
            elif key == 'original_id':
                frontmatter[key] = int(value) if value.isdigit() else value
            else:
                frontmatter[key] = value

    return frontmatter


def extract_category(title: str) -> Optional[str]:
    """
    ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ã€ã‚«ãƒ†ã‚´ãƒªã€‘ã‚’æŠ½å‡º

    Args:
        title: è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«

    Returns:
        ã‚«ãƒ†ã‚´ãƒªåï¼ˆã€ã€‘ãªã—ã®æ–‡å­—åˆ—ï¼‰ã€ãªã‘ã‚Œã°None
    """
    category_match = re.search(r'ã€(.+?)ã€‘', title)
    return category_match.group(1) if category_match else None


def count_words(content: str) -> int:
    """
    æœ¬æ–‡ã®æ–‡å­—æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆï¼ˆfrontmatterã‚’é™¤ãï¼‰

    Args:
        content: ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã®å†…å®¹

    Returns:
        æ–‡å­—æ•°
    """
    # frontmatterã‚’é™¤å»
    body = re.sub(r'^---\n.*?\n---\n', '', content, flags=re.DOTALL)

    # ç©ºç™½ãƒ»æ”¹è¡Œã‚’é™¤ã„ãŸæ–‡å­—æ•°
    body_cleaned = re.sub(r'\s+', '', body)

    return len(body_cleaned)


def generate_article_id(date_str: str, original_id: any) -> str:
    """
    è¨˜äº‹IDã‚’ç”Ÿæˆï¼ˆfc2_YYYY-MM-DD_NNNå½¢å¼ï¼‰

    Args:
        date_str: æ—¥ä»˜æ–‡å­—åˆ—ï¼ˆYYYY-MM-DDï¼‰
        original_id: FC2ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ID

    Returns:
        è¨˜äº‹ID
    """
    # original_idã‚’3æ¡ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
    id_num = str(original_id).zfill(3)
    return f"fc2_{date_str}_{id_num}"


def extract_article_metadata(file_path: Path, base_dir: Path) -> Dict:
    """
    1ã¤ã®FC2è¨˜äº‹ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º

    Args:
        file_path: è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        base_dir: data/raw/fc2_extracted/ã®ãƒ‘ã‚¹

    Returns:
        è¨˜äº‹ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¾æ›¸
    """
    content = file_path.read_text(encoding='utf-8')
    frontmatter = extract_frontmatter(content)

    title = frontmatter.get('title', file_path.stem)
    date_str = str(frontmatter.get('date', ''))
    original_id = frontmatter.get('original_id', 0)

    # ç›¸å¯¾ãƒ‘ã‚¹å–å¾—
    try:
        relative_path = file_path.relative_to(base_dir)
    except ValueError:
        relative_path = file_path

    article_id = generate_article_id(date_str, original_id)
    category = extract_category(title)
    word_count = count_words(content)

    # å¹´ã‚’æŠ½å‡º
    year = int(date_str.split('-')[0]) if date_str and '-' in date_str else None

    return {
        "id": article_id,
        "title": title,
        "date": date_str,
        "category": category,
        "word_count": word_count,
        "year": year,
        "original_id": original_id,

        "corpus_metadata": {
            "source_path": f"data/raw/fc2_extracted/{relative_path}",
            "tags": [],  # å°†æ¥çš„ã«åˆ†æã§è¿½åŠ 
            "quality_score": None,  # å°†æ¥çš„ã«ELOè©•ä¾¡ã§è¨­å®š
            "elo_rating": 1500,  # åˆæœŸå€¤
            "sampled": False,
            "reference_article": False
        },

        "rewrite_status": {
            "status": "pending",  # pending/in_progress/completed/deleted/archived
            "rewrite_score": None,  # 0-100ç‚¹
            "note_article_path": None,
            "rewrite_date": None,
            "rewrite_type": None,  # ã‚¿ã‚¤ãƒ ã‚«ãƒ—ã‚»ãƒ«å‹/æ–‡åŒ–å²æŠ½å‡ºå‹/å“²å­¦æ˜‡è¯å‹
            "deletion_reason": None,
            "archived_reason": None
        }
    }


def generate_statistics(articles: List[Dict]) -> Dict:
    """
    çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆ

    Args:
        articles: è¨˜äº‹ãƒªã‚¹ãƒˆ

    Returns:
        çµ±è¨ˆæƒ…å ±è¾æ›¸
    """
    total = len(articles)

    # çŠ¶æ…‹åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
    status_counts = {}
    for article in articles:
        status = article["rewrite_status"]["status"]
        status_counts[status] = status_counts.get(status, 0) + 1

    # å¹´åˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
    year_counts = {}
    for article in articles:
        year = article.get("year")
        if year:
            year_counts[year] = year_counts.get(year, 0) + 1

    # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã‚«ã‚¦ãƒ³ãƒˆ
    category_counts = {}
    for article in articles:
        category = article.get("category", "æœªåˆ†é¡")
        category_counts[category] = category_counts.get(category, 0) + 1

    return {
        "total": total,
        "by_status": status_counts,
        "by_year": dict(sorted(year_counts.items())),
        "by_category": dict(sorted(category_counts.items(), key=lambda x: x[1], reverse=True))
    }


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ãƒ‘ã‚¹è¨­å®š
    project_root = Path(__file__).parent.parent.parent
    fc2_dir = project_root / "data" / "raw" / "fc2_extracted"
    output_dir = project_root / "data" / "corpus"
    output_file = output_dir / "metadata.json"

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"FC2è¨˜äº‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {fc2_dir}")
    print(f"å‡ºåŠ›å…ˆ: {output_file}")

    # FC2è¨˜äº‹ã‚’ã™ã¹ã¦åé›†
    md_files = sorted(fc2_dir.glob("**/*.md"))
    print(f"\næ¤œå‡ºã—ãŸFC2è¨˜äº‹: {len(md_files)}ä»¶")

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
    articles = []
    errors = []

    for i, md_file in enumerate(md_files, 1):
        try:
            article = extract_article_metadata(md_file, fc2_dir)
            articles.append(article)

            if i % 100 == 0:
                print(f"å‡¦ç†ä¸­... {i}/{len(md_files)}")
        except Exception as e:
            errors.append({"file": str(md_file), "error": str(e)})
            print(f"ã‚¨ãƒ©ãƒ¼: {md_file} - {e}")

    print(f"\næŠ½å‡ºå®Œäº†: {len(articles)}ä»¶")

    if errors:
        print(f"ã‚¨ãƒ©ãƒ¼ä»¶æ•°: {len(errors)}ä»¶")

    # çµ±è¨ˆæƒ…å ±ç”Ÿæˆ
    statistics = generate_statistics(articles)

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONç”Ÿæˆ
    metadata = {
        "generated_at": datetime.now().isoformat(),
        "version": "1.0",
        "articles": articles,
        "statistics": statistics,
        "errors": errors
    }

    # å‡ºåŠ›
    with output_file.open('w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… metadata.json ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_file}")
    print(f"\nğŸ“Š çµ±è¨ˆæƒ…å ±:")
    print(f"  ç·è¨˜äº‹æ•°: {statistics['total']}ä»¶")
    print(f"  å¹´åˆ¥:")
    for year, count in statistics['by_year'].items():
        print(f"    {year}: {count}ä»¶")
    print(f"\n  ã‚«ãƒ†ã‚´ãƒªåˆ¥ï¼ˆä¸Šä½5ä»¶ï¼‰:")
    for category, count in list(statistics['by_category'].items())[:5]:
        print(f"    {category}: {count}ä»¶")


if __name__ == "__main__":
    main()
